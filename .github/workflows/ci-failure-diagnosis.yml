name: TensorZero CI Diagnosis Bot

on:
  workflow_run:
    workflows: ["Run Pytest"]
    types:
      - completed

permissions:
  contents: write
  pull-requests: write
  actions: read

jobs:
  gather-diff-and-generate-patch:
    if: ${{ github.event.workflow_run.conclusion == 'failure' }}
    runs-on: ubuntu-latest
    name: Gather failure context and generate PR
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha }}
          fetch-depth: 0

      - name: Gather PR metadata
        id: pr-info
        uses: actions/github-script@v7
        with:
          script: |
            const run = context.payload.workflow_run;
            const pr = run.pull_requests[0];

            if (!pr) {
              core.setFailed('No pull request found for the failing workflow run.');
              return;
            }
            core.setOutput('run-id', run.id);
      
      - name: Collect logs from the failed workflow run
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          mkdir -p diagnostics
          cd diagnostics
          gh run view ${{ steps.pr-info.outputs.run-id }} --log-failed > failed-jobs.log
          cd ..
          # print the contents of the logs directory
          ls -la diagnostics

      - name: Write TensorZero metrics configuration
        run: |
          echo "[functions.tensorzero_github_ci_bot]" > tensorzero.toml
          echo "type = \"chat\"" >> tensorzero.toml
          echo "" >> tensorzero.toml
          echo "[functions.tensorzero_github_ci_bot.variants.gpt_5]" >> tensorzero.toml
          echo "type = \"chat_completion\"" >> tensorzero.toml
          echo "model = \"openai::gpt-5\"" >> tensorzero.toml
          echo "" >> tensorzero.toml
          echo "[metrics.tensorzero_github_ci_bot_pr_merged]" >> tensorzero.toml
          echo "type = \"boolean\"" >> tensorzero.toml
          echo "optimize = \"max\"" >> tensorzero.toml
          echo "level = \"inference\"" >> tensorzero.toml
          echo "" >> tensorzero.toml
          echo "[metrics.tensorzero_github_ci_bot_diff_patched_successfully]" >> tensorzero.toml
          echo "type = \"boolean\"" >> tensorzero.toml
          echo "optimize = \"max\"" >> tensorzero.toml
          echo "level = \"inference\"" >> tensorzero.toml

      - name: Start TensorZero gateway
        run: |
          docker pull tensorzero/gateway:latest
          docker run -d --rm \
            --name tensorzero-gateway \
            -e TENSORZERO_CLICKHOUSE_URL=${{ secrets.CLICKHOUSE_URL }} \
            -e OPENAI_API_KEY=${{ secrets.OPENAIAPIKEY }} \
            -p 3000:3000 \
            --volume ./:/app \
            tensorzero/gateway:latest --config-file /app/tensorzero.toml

          for i in {1..20}; do
            curl -fsS http://localhost:3000/health && exit 0
            sleep 3
          done
          echo "Gateway never became ready" >&2
          exit 1

      - name: Call LLM to generate PR revision
        uses: shuyangli/t0-typescript-action/generate-pr-patch@main
        with:
          openai-api-key: ${{ secrets.OPENAIAPIKEY }}
          tensorzero-base-url: http://localhost:3000
          tensorzero-diff-patched-successfully-metric-name: tensorzero_github_ci_bot_diff_patched_successfully
          input-logs-dir: diagnostics
          output-artifacts-dir: debug-logs
          clickhouse-url: ${{ secrets.CLICKHOUSE_URL }}
          clickhouse-table: GitHubBotPullRequestToInferenceMap

      - name: Upload diagnostics bundle
        if: always()
        uses: actions/upload-artifact@v4
        continue-on-error: true
        with:
          name: ci-failure-diagnostics
          path: |
            diagnostics/
            debug-logs/

      - name: Stop TensorZero gateway
        if: always()
        run: docker stop tensorzero-gateway
        continue-on-error: true
